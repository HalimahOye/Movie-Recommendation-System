{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Quality checks\n",
    ".missing values \n",
    ".duplicate rows\n",
    ".basic data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(df):\n",
    "    \"\"\"\n",
    "    stores data quality metrics\n",
    "    \"\"\"\n",
    "    quality_report = {\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'duplicates': df.duplicated().sum(),\n",
    "        'total_rows': len(df),\n",
    "        'memory_usage': df.memory_usage().sum() / 1024**2  # in MB\n",
    "    }\n",
    "    return quality_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Data Types\n",
    "Converting string dates to datetime objects\n",
    "Identifying and converting numeric strings to actual numbers\n",
    "Ensuring categorical variables are properly encoded\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_datatypes(df):\n",
    "    for column in df.columns:\n",
    "        # Try converting string dates to datetime\n",
    "        if df[column].dtype == 'object':\n",
    "            try:\n",
    "                df[column] = pd.to_datetime(df[column])\n",
    "                print(f\"Converted {column} to datetime\")\n",
    "            except ValueError:\n",
    "                # Try converting to numeric if datetime fails\n",
    "                try:\n",
    "                    df[column] = pd.to_numeric(df[column].str.replace('$', '').str.replace(',', ''))\n",
    "                    print(f\"Converted {column} to numeric\")\n",
    "                except:\n",
    "                    pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Missing Values\n",
    "Using median imputation for numeric columns\n",
    "Applying mode imputation for categorical data\n",
    "Maintaining the statistical properties of the dataset while filling gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    # Handle numeric columns\n",
    "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numeric_columns) > 0:\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        df[numeric_columns] = num_imputer.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    # Handle categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_columns) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_columns] = cat_imputer.fit_transform(df[categorical_columns])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect and Handle Outliers\n",
    "You need domain knowledge to decide on “what” might actually be outliers.\n",
    "\n",
    "Here's an approach using the Interquartile Range (IQR) method:\n",
    "\n",
    "Calculate Interquartile Range (IQR) for numeric columns\n",
    "Identify values beyond 1.5 * IQR from quartiles\n",
    "Apply capping to extreme values rather than removing them\n",
    "This preserves data while managing extreme values.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    outliers_removed = {}\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Count outliers before removing\n",
    "        \n",
    "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)].shape[0]\n",
    "        \n",
    "        # Cap the values instead of removing them\n",
    "        df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "        \n",
    "        if outliers > 0:\n",
    "            outliers_removed[column] = outliers\n",
    "            \n",
    "    return df, outliers_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the Results\n",
    " \n",
    "After cleaning, we need to verify that our pipeline worked as expected:\n",
    "\n",
    "Confirm no remaining missing values\n",
    "Check for any remaining duplicates\n",
    "Validate data integrity and consistency\n",
    "Generate a comprehensive cleaning report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_cleaning(df, original_shape, cleaning_report):\n",
    "    validation_results = {\n",
    "        'rows_remaining': len(df),\n",
    "        'missing_values_remaining': df.isnull().sum().sum(),\n",
    "        'duplicates_remaining': df.duplicated().sum(),\n",
    "        'data_loss_percentage': (1 - len(df)/original_shape[0]) * 100\n",
    "    }\n",
    "    \n",
    "    # Add validation results to the cleaning report\n",
    "    cleaning_report['validation'] = validation_results\n",
    "    return cleaning_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, let's put it all together in a complete pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_cleaning_pipeline(df):\n",
    "    # Store original shape for reporting\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    # Initialize cleaning report\n",
    "    cleaning_report = {}\n",
    "    \n",
    "    # Execute each step and collect metrics\n",
    "    cleaning_report['initial_quality'] = check_data_quality(df)\n",
    "    \n",
    "    df = standardize_datatypes(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df, outliers = remove_outliers(df)\n",
    "    cleaning_report['outliers_removed'] = outliers\n",
    "    \n",
    "    # Validate and finalize report\n",
    "    cleaning_report = validate_cleaning(df, original_shape, cleaning_report)\n",
    "    \n",
    "    return df, cleaning_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(            userId  movieId  rating     timestamp\n",
       " 0           5163.0  19779.5     4.0  1.518350e+09\n",
       " 1         106343.0      5.0     4.5  1.206239e+09\n",
       " 2         146790.0   5459.0     5.0  1.076216e+09\n",
       " 3         106362.0  19779.5     2.0  1.423043e+09\n",
       " 4           9041.0    366.0     3.0  8.333758e+08\n",
       " ...            ...      ...     ...           ...\n",
       " 10000033  136395.0  19779.5     5.0  1.521235e+09\n",
       " 10000034  140078.0    553.0     3.0  1.002581e+09\n",
       " 10000035  154807.0  19779.5     4.0  1.227675e+09\n",
       " 10000036   85805.0    327.0     4.0  1.479922e+09\n",
       " 10000037  139457.0   1009.0     4.0  8.589849e+08\n",
       " \n",
       " [10000038 rows x 4 columns],\n",
       " {'initial_quality': {'missing_values': {'userId': 0,\n",
       "    'movieId': 0,\n",
       "    'rating': 0,\n",
       "    'timestamp': 0},\n",
       "   'duplicates': 0,\n",
       "   'total_rows': 10000038,\n",
       "   'memory_usage': 305.1770668029785},\n",
       "  'outliers_removed': {'movieId': 2359041, 'rating': 468784},\n",
       "  'validation': {'rows_remaining': 10000038,\n",
       "   'missing_values_remaining': 0,\n",
       "   'duplicates_remaining': 90738,\n",
       "   'data_loss_percentage': 0.0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automated_cleaning_pipeline(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000038, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
